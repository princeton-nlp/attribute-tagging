t5-large (run.sslurm)
--------
* [node006] batch_size=1, acc=08 -> Cuda out of memory
* [node006] batch_size=1, acc=32 -> Cuda out of memory
* [node006] batch_size=4, acc=08 -> Cuda out of memory
* [node006] batch_size=1, acc=32; [fp16, ckpt] -> Cuda out of memory

t5-large + no trainer (run_no.sh)
--------
* batch_size=100, acc=32 -> Tried to allocate 3.12 GiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 2.04 GiB free; 7.77 GiB reserved in total by PyTorch)
* batch_size=100, acc=64 -> Tried to allocate 3.12 GiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 2.04 GiB free; 7.77 GiB reserved in total by PyTorch)
* batch_size=150, acc=64 -> Tried to allocate 3.12 GiB (GPU 0; 10.76 GiB total capacity; 7.75 GiB already allocated; 2.04 GiB free; 7.77 GiB reserved in total by PyTorch)
* batch_size=1  , acc=64 -> Tried to allocate 64.00 MiB (GPU 0; 10.76 GiB total capacity; 9.60 GiB already allocated; 42.44 MiB free; 9.75 GiB reserved in total by PyTorch)

t5-large + no trainer + accelerate (run_acc.sh)
--------
* 
--------
1. `accelerate config` runs fine, but then breaks on `accelerate test` due to some torchrun error -> fails on ./run_acc.sh

general
--------
* `accelerate config` output saved to ~/.cache/huggingface/accelerate
* Interactive Mode: gonode --gres=gpu:4 --mem=32G -x node[009-016],node[030-031],node[200-201],node[914]
* Make sure batch size / # of gpus is evenly divisible